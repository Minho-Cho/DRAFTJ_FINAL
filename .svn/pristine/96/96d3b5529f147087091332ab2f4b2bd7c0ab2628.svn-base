# STT #

IBM Watson Speech to Text 서비스는 음성 인식 기능을 사용하여 아랍어, 영어, 스페인어, 프랑스어, 브라질 포르투갈어, 일본어, 한국어 및 만다린어를 텍스트로 변환합니다.
IBM® Speech to Text 서비스는 응용 프로그램에 음성인식 기능을 추가하는 데 사용할 수있는 응용 프로그램 프로그래밍 인터페이스 (API)를 제공합니다. 
이 서비스는 기계 지능을 활용하여 사람의 음성을 정확하게 녹음합니다.
이 서비스는 문법 및 언어 구조에 대한 정보를 오디오 신호의 구성에 대한 지식과 결합합니다.
(더 많은 연설이 들릴 때 지속적으로 텍스트가 업데이트됩니다.)

■ 장점
1) 강력한 실시간 음성 인식
- 7개 언어의 실시간 오디오를 자동으로 스크립트화
- 다양한 오디오 형식 및 프로그래밍 인터페이스 지원
- 저음질의 오디오에서도 발언 내용을 신속하게 파악하여 텍스트로 작성

2) 정확도 높은 음성 엔진
- 제품 이름, 관련 주제, 사람 이름 등 가장 중요한 항목을 언어 및 컨텐츠에 맞는 모델로 구성하여 정확도 향상
- 키워드를 지정해주면 실시간으로 찾아내며 정확도 및 신뢰도가 높음
- (Only 미국영어, 스페인어, 일본어) 오디오에서 다양한 화자 인식

■ 입력기능
1) 오디오 형식
- Opus 또는 Vorbis 코덱, MP3 (또는 MPEG), WAV (Waveform Audio File Format), FLAC (Free Lossless Audio Codec), 선형 16 비트 펄스 코드 변조 (PCM), mu-law (또는 u-law) 오디오 및 기본 오디오
2) 언어 및 모델
- 대부분의 언어에서 광대역 또는 협 대역 모델을 사용하여 오디오 녹음 가능 
- 최소 16kHz로 샘플링 된 오디오에는 광대역을 사용권장
- 최소 8kHz로 샘플링 된 오디오에는 협 대역을 사용권장
3) 오디오 전송 
- 100MB의 오디오를 서비스에 전달 가능
- 오디오는 연속적인 스트림으로 전달하거나 한 번에 모든 데이터를 전달하는 단발 전송으로 전달 가능
- 스트리밍을 사용하면 다양한 시간 제한을 적용하여 리소스 보존

■ 출력 기능
1) 스피커 레이블(Speaker labels)
- (Only 미국 영어, 스페인어 또는 일본어) 오디오의 화자를 인식
2) 키워드 표시(Keyword spotting)
- 지정된 키워드 문자열과 사용자가 정의한 신뢰 수준(Confidence level)을 만족하는 구문을 식별
- 키워드 스포팅은 오디오의 개별 구문이 전체 텍스트보다 더 중요 할 때 특히 유용
3) 단어 기록 및 중간 기록 결과 (Maximum alternatives & Interim results)
- 최대 대안은 다양하고 가능한 텍스트를 제공
- 중간 결과는 기록 진행 도중에 가설 제시
- 두 경우 모두 신뢰(Confidence) 수준이 가장 높은 최종 결과 표시
4) 단어 대체(Word alternatives)
- 음향과 유사한 다른 단어를 제안 (베타 기능.)
5) 신뢰도(Word confidence)
- 각 단어에 대한 신뢰 수준을 반환
6) 타임 스탬프(Word timestamps)
- 각 단어의 시작과 끝의 타임 스탬프를 반환
7) 욕설 필터링(Profanity filtering)
- (Only 미국영어) 비속어 검열
8) 스마트 포맷(Smart formatting)
- (Only 미국영어) 날짜, 시간, 숫자, 통화 값, 전화 번호 및 인터넷 주소를 일반적인 양식으로 변환

■ 인터페이스
1) 웹소켓(Websocket) 인터페이스
- 서비스와 persistent, full-duplex 연결을 설정
- 효율적이고, 낮은 대기 시간 및 높은 처리량 구현 제공
2) HTTP REST 인터페이스
- 서비스에 대한 세션리스(sessionless) 및 세션기반(session-based) 호출을 모두 지원
3) 비동기(Asynchronous) HTTP 인터페이스
- 서비스에 non-blocking 호출 제공
- 콜백 URL을 등록하여 결과를 수신하거나 작업 상태 및 결과에 대한 서비스를 폴링 가능

■ 사용자정의(Customization) 인터페이스
1) 사용자 지정 언어 모델(Custom language models)
- 기본 모델에 대한 도메인 별 단어를 정의 가능
- 의학 및 법률과 같은 도메인 영역에 관련된 특수 용어로 서비스의 기본 어휘를 확장
2) 사용자 지정 음향 모델(Custom acoustic models) 
- 환경 및 스피커의 음향 특성에 대한 기본 모델을 적용 가능
- 사용자 지정 음향 모델은 특정 음향 특성에 대한 음성을 인식하는 서비스의 기능을 개선

■ 개발 지원
1) SDK
- 여러 SDK (Software Development Kit)를 사용하여 다양한 언어 및 환경에서 응용 프로그램 개발을 단순화 가능
- SDK는 Node.js, Java 및 Python을 비롯한 많은 인기있는 프로그래밍 언어 및 플랫폼에서 사용 가능 
2) CORS
- CORS를 사용하면 웹 페이지가 외부 도메인에서 직접 리소스 요청 가능
* 모든 JSON 응답 내용을 UTF-8 문자 세트로 반환

■ 프로그래밍 모델
1) Direct interaction
- 클라이언트가 직접 서비스에 오디오 스트리밍
- IBM Cloud의 서비스에 대한 인증 토큰 획득 후 서비스와 직접 통신합
- 직접 상호 작용은 서비스 자격 증명을 사용하여 토큰 획득
2) Relaying via proxy
- IBM® 클라우드에있는 프록시 응용 프로그램을 통해 클라이언트와 서비스 간 데이터(요청, 오디오 및 결과) 교환
- 클라이언트는 모든 요청을 프록시 응용 프로그램을 통해 전달 
- 자격 증명을 사용하여 서비스 인증

■ 응용 프로그램 개발을위한 고려 사항
1)오디오 품질 
- 음성 인식은 입력 오디오 품질에 매우 민감 할 수 있습니다. 
- 데모 애플리케이션을 실험하거나 서비스를 사용하는 애플리케이션을 제작할 때 입력 오디오 품질이 가능한 한 좋은지 확인하십시오.
- 최상의 정확성을 얻으려면 가능하면 항상 음성 기반 마이크 (예 : 헤드셋)를 사용하고 필요할 경우 마이크 설정을 조정하십시오.
- 노트북의 내장 마이크를 사용하지 마십시오.
2) 모델 선택
- 올바른 모델을 선택하는 것이 중요합니다.
- 지원되는 언어의 경우이 서비스는 광대역 및 협 대역의 두 가지 모델을 지원합니다.
- IBM®에서는 반응 형 실시간 응용 프로그램에 광대역 모델을 사용하고 전화 음성을 오프라인으로 디코딩 할 때 협 대역 모델을 사용할 것을 권장합니다. 
* 연설을 텍스트로 변환하는 것은 완벽하지 않을 수 있습니다. 
- 지난 몇 년 동안 엄청난 발전이있었습니다. 
- 오늘날 음성 인식 기술은 많은 도메인 및 응용 프로그램에서 성공적으로 사용되고 있습니다. 
- 그러나 오디오 품질 이외에도 음성 인식 시스템은 지역 악센트 및 발음 차이와 같은 사람의 말의 뉘앙스에 민감하며 항상 오디오 입력을 올바르게 녹음하지 못할 수 있습니다.

■ FAQ
기본 사용법
1. 내 오디오 파일이 100MB보다 큽니다. 서비스가 수용하기에는 너무 큽니다. 파일을 처리할 수 있는 방법이 있습니까?
- 한 가지 해결책은 수동으로 파일을 작은 조각으로 나누는 것입니다. 또는 오디오 품질이 저하될 수는 있지만 Sound eXchange(sox.sourceforge.net) 또는 FFmpeg(www.ffmpeg.org)와 같은 도구를 사용하여 파일을 압축할 수 있습니다.
2. 한 시간 분량의 오디오 파일을 서비스에 전달했지만 텍스트 변환이 완료될 때까지 결과를 볼 수 없었습니다. 결과를 더 빨리 볼 수있는 방법은 없습니까?
- HTTP 세션 또는 WebSocket 인터페이스를 사용하고, interim_results 파라미터를 true로 설정하십시오. 기본적으로 서비스는 오디오에서 긴 무음을 감지한 경우에만 결과를 반환합니다. 중간 결과 편을 참조하십시오.
3. 내 오디오 파일에는 다양한 길이로 아무말도 없는 부분이 존재합니다. 이에 서비스는 어떻게 응답합니까?
- 서비스는 스트림이 끝나거나 타임아웃이 발생할 때까지, 전체 오디오 스트림을 텍스트로 변환합니다. 입력 내용에 해당 부분이 포함되어 있으면, 텍스트 변환 결과에 일시 중지를 나타내는 여러 transcript 요소들이 포함될 수 있습니다. transcript 요소를 연결하여 오디오 스트림의 전체를 다시 연결할 수 있습니다. 예전에 사용된 인식 요청을 시작하는 모든 메소드들은 하나의 continuous 파라미터를 포함하고 있었습니다. 이 파라미터는 ture로 설정할 경우, 연설이 끝날 때(일반적으로 침묵이 발생할 경우), 서비스가 녹음을 멈추지 않도록 처리할 수 있었습니다. 그러나, 현재 서비스는 일시 중지 시에도 녹음을 중지하지 않으며, 해당 파라미터는 모든 메소드에서 제거되었습니다.
4. 문구가 끝나는 시점을 결정하는 시간을 지정하거나, 일시 중지 간격을 변경할 수 있습니까?
- 불가능합니다. 해당 기능은 현재 서비스에서는 제공되지 않습니다.
5. 산출 대본이 여러 개로 작성되었는데, 어떤 것을 사용해야 합니까?
- JSON final 속성이 true 인 대본을 사용하십시오. 속성이 false인 것은 중간 버전이므로 무시하십시오.

HTTP 세션
1. 왜 세션을 사용해야 합니까?
- 세션을 사용하면 동일한 서버(트랜스크립션 엔진)에 여러 개의 요청을 보내기가 수월합니다. 세션을 사용할 경우 한 가지 이점은, 새 연결을 각 호출에 대해 초기화할 필요가 없으므로 이후 호출의 대기 시간이 단축된다는 점입니다. 사용자 지정 모델을 사용하면 세션을 통해 대기시간이 또한 줄어 듭니다. 동일한 서버를 사용하면, 후속 인식 요청의 정확도를 높일 수도 있습니다. WebSocket 인터페이스를 사용하면 이러한 동일한 이점을 누릴 수 있습니다.
2. 세션은 쿠키를 어떻게 사용합니까?
- 세션 기반 메소드를 사용할 때, 세션을 생성하는 초기 POST 세션 요청은 Set-Cookie 응답 헤더에 쿠키를 반환합니다. 해당 세션을 사용하는 각 호출마다 이 쿠키를 반환해야 합니다. 자세한 내용은 세션에서 쿠키 사용하기 편을 참조하십시오.
3. 세션과 쿠키를 같이 사용할 경우와 관련된 요청 실패는 어떻게 진단합니까?
- 쿠키 사용과 관련된 두 가지 일반적인 오류는 세션 기반 요청에서 쿠키를 설정하고 잘못된 쿠키를 전달하는 것을 잊어버리는 경우입니다. 쿠키 전달을 소홀히하면 Cookie must be set (쿠키가 설정되어야함) 메시지와 함께 HTTP 상태 코드 400이 반환됩니다. 유효하지 않은 쿠키를 전달하면, 서비스는 Session does not exist (세션 존재하지 않음) 메시지와 함께 HTTP 상태 코드 404를 반환합니다. 세션에서 쿠키 사용하기편에서는 세션 쿠키와 세션 ID가 일치하는지 확인하는 방법을 설명하며, 해당 내용은 상기한 두 번째 오류의 진단에 도움을 줄 수 있으니 참고하시기 바랍니다.

WebSocket API
1. WebSocket API를 사용하는 경우 세션을 사용해야합니까?
- 아니요. WebSocket 인터페이스를 사용하면 WebSocket이 서비스에 연결되며, 연결은 해당 세션이 됩니다. WebSocket 연결은 세션의 모든 이점을 제공합니다. 예를 들어, 단일 소켓, 전이중 통신 채널을 제공합니다. 클라이언트는 서비스에 요청을 보내고 비동기식으로 단일 연결을 통해 결과를 수신합니다. 또한 WebSocket 프로토콜은 매우 가볍기 때문에 네트워크 대기시간을 줄여줍니다. 관련 정보는 WebSocket 인터페이스 사용하기 편을 참조하십시오.
2. 마지막 가설을 받기 전에 내 WebSocket 연결 시간이 초과되었습니다. 어떻게 처리해야 합니까?
- 정지 메시지 또는 비어 있는 2 진 메시지를 서비스에 보내 오디오의 끝을 알리는지 확인하십시오. 인식 요청 종료하기 편을 참조하십시오.

인식 정확도
1. 최고의 인식 정확도를 달성하기 위해 오디오와 함께 사용해야 하는 모델은 광대역 또는 협대역 중 어떤 것인가요?
- 오디오의 주파수 내용을 고려해야합니다. 오디오의 샘플링 속도(및 언어)와 일치하는 모델을 사용하십시오. 모델 및 최소 샘플링 속도에 대한 설명은 언어 및 모델 편을 참조하십시오.
- 일반적으로 샘플링 속도가 8KHz 이하이면 협대역 모델을 사용하십시오. 그렇지 않으면 광대역 모델을 사용하십시오. 또한 샘플링 속도가 8KHz보다 높지만, 분광사진 검사에서 4KHz 이상의 주파수가 표시되지 않으면 8KHz로 다운 샘플링하는 것이 좋습니다. 이 설명이 부족한 경우, Stack Overflow 또는 dW Answers의 Watson 포럼에 질문을 올려주시기 바랍니다.
2. 인식 정확도를 높이기 위해 오디오 파일을 업 샘플링하여 광대역 모델로 보냈지만 예상한 결과가 도출되지 않았습니다. 왜 그런걸까요?
- 업 샘플링은 도움이 되지 않을 수 있으며, 실제로 성능을 저하시킬 수 있습니다. 협대역 모델은 8KHz로 샘플링된 오디오로 구축되며, 광대역 모델은 16KHz로 샘플링된 오디오로 구축됩니다. 16KHz의 샘플링 속도는 샘플링된 신호에 존재하는 최대 주파수가 8kHz라는 것을 의미합니다. 즉, 원본 신호는 16KHz의 속도로 샘플링하기 전에 8KHz에서 필터링해야 합니다. 그렇지 않으면, 앨리어싱(aliasing)이라고 알려진 현상으로 인해 성능 저하가 발생합니다(그 원인을 이해하려면 Nyquist 주파수 내용을 참조). 따라서, 광대역과 협대역을 비교할 때, 광대역 모델은 4-8 KHz 범위의 정보를 기대하지만 협대역 모델은 4 KHz보다 작거나 같은 범위의 정보를 찾습니다.
- 애초에 협대역 주파수에서 샘플링된 오디오 내의 정보는 0-4 KHz 범위로 제한되며 업 샘플링 된 오디오는 광대역 모델에서 예상한 범위의 정보가 부족하게 되어, 인식 정확도가 떨어집니다. 또한, 협대역 샘플의 예상 범위에서 발견된 정보는 광대역 샘플의 동일한 범위에서 발견되는 정보와 질적으로 다릅니다. 모델에 대한 훈련 데이터는 매우 다른 성격의 채널(협대역 모델의 경우 전화 통신)에서 파생되었으며, 모델은 훈련을 받은 채널의 특성을 반영합니다.
- 유용한 비교 방법은 대형 평면 HDTV에서 VHS 테이프를 보는 것을 상상해 보는 것입니다. 고화질 장치에서 테이프를 재생할 때, 실제로 스트림에 새로운 정보를 추가할 수 없으므로 이미지가 흐릿하게 표시됩니다. 단순히 형식이 더 나은 장치와 호환되도록 하기 때문입니다. 업 샘플링 오디오도 이와 마찬가지로 이해할 수 있을 것입니다.
3. 나의 응용 사례는 화자가 질문에 "예" 또는 "아니오"로 대답하는지를 판단하는 것입니다. 그러나 산출 대본은 항상 예 또는 아니오를 반환하지 않으며, 유사한 발음이 나는 다른 단어들을 포함할 수도 있더군요(no라고 대답했을 때, nine을 표시). 이 경우 어떻게 인식을 향상시킬 수 있습니까?
- 대체 대본 결과 및 대체 단어들을 점검해 보면, 예상된 단어를 찾을 수 있는 경우가 종종 있습니다. 해당 내용에는 최종 산출 대본에 포함된 단어를 대체할 수 있는(하지만 결국 일부 이유로 선택되지는 않은) 대안/대체 단어들이 포함될 수 있습니다. 다음 두 가지 방법 중 하나를 이용하면, 이러한 대안 버전들을 볼 수 있습니다.
- max_alternatives 파라미터에 하나의 정수를 지정하십시오. 예를 들어 5를 지정하면 해당 대본의 개수를 볼 수 있습니다. 최대 대안 버전 편을 참고하십시오.
- word_alternatives_threshold 파라미터의 확률을 지정하십시오. 예를 들어 서비스의 신뢰도가 50%인 대체 단어를 보려면 0.5를 지정하십시오. 대체 단어 편을 참조하십시오.
- 일반적으로 Speech to Text 서비스는 주변 소음에 매우 민감합니다. 엔진 소음, 구동 장치, 길거리의 소음, 주변 대화 등은 산출물의 정확성을 크게 떨어뜨릴 수 있습니다. 또한 휴대기기 및 태블릿에 일반적으로 설치되는 마이크가 부적절한 것일 수 있습니다. 전문가용 마이크를 사용하여 더 나은 품질의 오디오를 확보할 경우, 최상의 품질을 확보할 수 있습니다. 또한 스피커가 마이크에서 멀어질수록 서비스의 정확성이 떨어지게 됩니다. 예를 들어, 10피트 거리에서 녹음된 오디오를 사용할 경우, 서비스는 적절한 결과를 내기 어렵습니다.

사용자 정의 기능
1. 스스로 작성한 전문 어휘를 서비스에 추가할 수 있습니까?
- 예. 이 서비스의 기본 어휘에는 일반 대중이 일상적으로 대화하는 데에 사용되는 많은 단어가 포함되어 있습니다. 이 모델은 다양한 애플리케이션에 대해 충분히 정확한 인식을 제공하지만, 특정 도메인과 관련된 특정 용어에 대한 지식은 부족할 수 있습니다. 이러한 유형의 단어들을 OOV(Out Of of Vocabulary) 단어라고 합니다.
- 언어 모델 사용자 정의 인터페이스를 사용하면 특수 분야에 대한 음성 인식 정확도를 높일 수 있습니다. 사용자 정의 기능을 사용하여 기본 모델의 어휘를 확장 및 조정하여 도메인 특정 데이터 및 용어를 포함시킬 수 있습니다. 사용자 정의 기능 사용하기 편을 참조하십시오. 사용자 정의 기능은 현재 미국 영어와 일본어에서만 지원됩니다.

2. 많은 사용자 정의 메소드가 비동기식입니다. 작업이 끝난뒤 결과를 확인하려면 어떻게합니까?
- 세 가지의 사용자 지정 작업이 비동기식으로 이루어집니다. 작업 결과를 확인하는 방법은 아래 지침을 참고하시기 바랍니다.
- POST/v1/customizations/{customization_id}/corpora/{corpus_name} 메소드를 사용하여 말뭉치를 모델에 추가하라는 요청의 처리 상태를 확인할 수 있습니다. 말뭉치 요청 모니터링하기 편을 참조하십시오.
- POST/v1/customizations/{customization_ID}/words 메소드를 사용하여 모델에 단어를 추가하라는 요청의 상태를 확인할 수 있습니다. 추가 단어 요청 모니터링하기 편을 참조하십시오.
- POST/v1/customizations/{customization_id}/train 메소드를 사용하여 모델을 훈련시키라는 요청의 상태를 확인할 수 있습니다. 모델 훈련 요청 모니터링하기 편을 참조하십시오.
- Python 또는 쉘 스크립트에서 조작을 모니터하는 샘플 코드는 예제 스크립트 편을 참조하십시오.

# NLU #

IBM Watson ™ Natural Language Understanding을 통해 개발자는 범주, 개념, 감정, 엔티티, 키워드, 메타 데이터, 관계, 의미 론적 역할 및 정서를 포함한 텍스트 입력의 의미 론적 기능을 분석 할 수 있습니다.

■ 특징
1) 정형 데이터 및 비정형 데이터로부터 인사이트 발굴
- 개념, 개체, 키워드, 범주, 관계, 의미론적 역할 등의 컨텐츠로부터 메타데이터를 추출하기 위해 텍스트 분석
2) 정서 및 감정 이해
- 어떤 문서에 대한 전반적인 정서 및 감정 또는 텍스트에 포함된 키워드와 관련하여 의도된 정서 및 감정을 파악하여 심층 분석 가능
3) 다국어 이해
- NLU는 9개 언어의 텍스트를 이해하며 Watson Knowledge Studio를 통한 맞춤 구성 가능

■ 서비스 기능
1) 카테고리(Categories)
- 5 단계 분류 계층 구조를 사용하여 컨텐츠 분류
2) 컨셉(Concepts)
- 본문에서 직접 참조되지 않는 상위 개념을 확인
3) 감정(Emotion)
- (Only 영어)
- 특정 목표 문구 또는 문서 전체로 전달되는 감정을 분석 
- 또한 서비스에서 자동으로 감지되는 엔티티 및 키워드에 대해 감정 분석 사용 가능
4) 개체(Entities)
- 콘텐츠에 언급 된 사람, 장소, 이벤트 및 기타 유형의 엔터티 파악
5) 관계(Relations)
- 두 엔티티가 관련되어있을 때를 인식하고 관계 유형을 식별
6) 키워드(Keywords)
- 컨텐츠 키워드 파악
7) 메타 데이터(Metadata)
- HTML 및 URL 입력의 경우 웹 페이지 작성자, 페이지 제목 및 게시 날짜를 파악
8) 의미역(Semantic Roles)
- 문장을 주체 - 행동 - 목표 형태로 분석하고, 주체 또는 행동의 대상인 주체와 키워드 식별
9) 정서(Sentiment)
- 특정 문구에 대한 감정과 문서 전체에 대한 정서를 분석
- 또한 해당 기능에 대한 감정 옵션을 사용하여 탐지 된 항목 및 키워드에 대한 정서 정보 획득

# 이용요금 #
■ Assistant
- 과금 기준 : 월 API Call 수
- 3원/API호출1회, 처음 1000 Call 무료
- 20워크스페이스/2000 인텐트 생성 가능

■ NLC
- 과금 기준 : 월 API Call 수, 월 Training 수, 월 인스턴스 개수
- 5.5원/API Call, 월 1000 Call 무료
- 31,200원/Instance(NLC Classifier), 월 1 Instance 무료
- 4,680원/Event(Training event), 월 4 Event (Training event) 무료

■ NLU
- 과금 단위 : Item 호출 수
- 4.7원/Item, 월 1000 Item 무료
- n개(10,000자 기준 1개)와 기능의 수를 기반으로 Item 별로 요금이 과금됩니다.
- 예를 들어, 15,000자의 텍스트에서 2개의 기능 (Entity, Sentiment)을 사용하면 2x2=4개의 NLU Item 으로 요금이 계산됩니다.

■ STT
- 고금 기준 : 월 음성인식 시간
- 기본 모델 사용 시 : 26원/분, 처음 1000분 무료
- 커스텀 언어 모델 적용 시 : 기본 모델 사용료(26원/분) + 40원/분, 처음 1000분 무료
- 커스텀 음향 모델 적용 시  베타 서비스, 가격 미정 


# 앞으로의 개선점 #
1) 음성인식(STT) 학습 모델  <= 사용자 지정 언어 모델 적용 필요
(이슈사항 : AIBRIL 음성인식(STT) 사용자 지정 언어 모델 한국어 미지원)
=> AIBRIL 한국어 지원 일정 확인 필요
2) 자연어분류(NLC) 회의록 핵심문장(의사결정 문구) 학습을 통한 추정결론 제안 
3) 대화(Assistant) 와 Knowledge DB (WEX, 용어집 등) 연계필요

# SWOT #
- 강점(S)
1) 학습능력 : 지속적으로 개선
2) 서비스확장성 : 기반기술 활용하여 다양한 서비스 발굴 가능 
3) 개발용이성 : AIBRIL 서비스 활용하여 편리하게 개발 가능
4) DT 기술
- 약점(W)
1) 예측 어려움 : 딥러닝 기반으로 서비스의 결과를 예측하기 어려움
2) 비용 : AIBRIL 이용요금 (종량제 과금정책)
- 기회(O)
1) Watson 적용 프로젝트 성공 사례
2) Digital Workplace
3) 사내 DT 사업 장려 분위기
- 위기(T)
1) AIBRIL 의존도 : AIBRIL 정책 등에 종속적
